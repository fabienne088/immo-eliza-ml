{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "df = pd.read_csv(\"../data/cleaned_properties.csv\")\n",
    "\n",
    "# Display the head\n",
    "display(df.head())\n",
    "df.shape\n",
    "df.columns\n",
    "\n",
    "# Filter the DataFrame for values APARTMENT and APARTMENT_BLOCK\n",
    "df_house1 = df[df[\"property_type\"] == \"HOUSE\"]\n",
    "df_house2 = df_house1[df_house1['subproperty_type'] != 'APARTMENT_BLOCK']\n",
    "\n",
    "df_house = df[(df[\"property_type\"] == \"HOUSE\") & (df['subproperty_type'] != 'APARTMENT_BLOCK')]\n",
    "\n",
    "df_house.head()\n",
    "print(df_house.info())\n",
    "print(df_house.shape)\n",
    "\n",
    "df_house[\"subproperty_type\"].unique()\n",
    "print(df_house[\"locality\"].unique())\n",
    "df_house.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating variables X and y: define the target and the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name X and y (specific columns=subset(houses))\n",
    "X = df_house.drop(columns=['price', 'subproperty_type', 'property_type', 'zip_code', 'locality', 'construction_year', 'cadastral_income'])\n",
    "y = df_house['price']\n",
    "\n",
    "# Print shape\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y-shape: \", y.shape)\n",
    "\n",
    "# Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imputing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values:\n",
    "- numerical: mean\n",
    "- categorical: most frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: Imputing missing values\n",
    "\n",
    "Impute missing values in both X_train and x_test\n",
    "\n",
    "So, the correct order of preprocessing steps is:\n",
    "- Impute missing values\n",
    "- Encode categorical columns\n",
    "- Rescale numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def impute_data(X_train):\n",
    "    \"\"\"\n",
    "    Imputes missing values in both numerical and categorical columns of the input DataFrame using SimpleImputer.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input DataFrame containing columns with missing values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Select columns with numerical and categorical data\n",
    "    numeric_cols = X_train.select_dtypes(exclude='object').columns.tolist()\n",
    "    categorical_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Impute missing values for numerical columns\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')  \n",
    "    X_train[numeric_cols] = numeric_imputer.fit_transform(X_train[numeric_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')  \n",
    "    X_train[categorical_cols] = categorical_imputer.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "    return X_train\n",
    "\n",
    "# Example usage:\n",
    "#X_train_imputed = impute_data(X_train)\n",
    "print(X_train.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def impute_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Imputes missing values in both numerical and categorical columns of the input DataFrames using SimpleImputer.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Training DataFrame containing columns with missing values.\n",
    "    X_test : pandas DataFrame\n",
    "        Testing DataFrame containing columns with missing values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Training and testing DataFrames with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Select columns with numerical and categorical data in training set\n",
    "    numeric_cols_train = X_train.select_dtypes(exclude='object').columns.tolist()\n",
    "    categorical_cols_train = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Select columns with numerical and categorical data in testing set\n",
    "    numeric_cols_test = X_test.select_dtypes(exclude='object').columns.tolist()\n",
    "    categorical_cols_test = X_test.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Impute missing values for numerical columns in training set\n",
    "    numeric_imputer_train = SimpleImputer(strategy='mean')  \n",
    "    X_train[numeric_cols_train] = numeric_imputer_train.fit_transform(X_train[numeric_cols_train])\n",
    "\n",
    "    # Impute missing values for categorical columns in training set\n",
    "    categorical_imputer_train = SimpleImputer(strategy='most_frequent')  \n",
    "    X_train[categorical_cols_train] = categorical_imputer_train.fit_transform(X_train[categorical_cols_train])\n",
    "\n",
    "    # Impute missing values for numerical columns in testing set\n",
    "    numeric_imputer_test = SimpleImputer(strategy='mean')  \n",
    "    X_test[numeric_cols_test] = numeric_imputer_test.fit_transform(X_test[numeric_cols_test])\n",
    "\n",
    "    # Impute missing values for categorical columns in testing set\n",
    "    categorical_imputer_test = SimpleImputer(strategy='most_frequent')  \n",
    "    X_test[categorical_cols_test] = categorical_imputer_test.fit_transform(X_test[categorical_cols_test])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "# Apply imputing to training and test data\n",
    "X_train_encoded, X_test_encoded = impute_data(X_train.copy(), X_test.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: Encode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_data(X_train):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in the input DataFrame using OneHotEncoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input DataFrame containing categorical columns to be encoded.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        DataFrame with categorical columns encoded using one-hot encoding.\n",
    "    \"\"\"\n",
    "    # Select the columns with categorical values\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Initialize the encoder\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_array = encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "    # Convert the encoded array into a DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # Concatenate the encoded DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([X_train, encoded_df], axis=1)\n",
    "\n",
    "    # Drop the original categorical columns if needed\n",
    "    result_df.drop(columns=categorical_cols, axis=1, inplace=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "    print(result_df.isna().sum().sort_values(ascending=False)) \n",
    "\n",
    "# Example usage:\n",
    "# X_train_encoded = encode_data(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot_encode(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in the input DataFrames using OneHotEncoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input training DataFrame.\n",
    "    X_test : pandas DataFrame\n",
    "        Input test DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple of pandas DataFrames\n",
    "        Encoded training and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Select the columns with categorical values\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Initialize the encoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Fit and transform on training data\n",
    "    X_train_ohe = encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "    # Transform test data\n",
    "    X_test_ohe = encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "    # Convert the encoded data into DataFrames\n",
    "    X_train_ohe_df = pd.DataFrame(X_train_ohe.toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "    X_test_ohe_df = pd.DataFrame(X_test_ohe.toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # Drop original categorical columns from both training and test data\n",
    "    X_train.drop(columns=categorical_cols, inplace=True)\n",
    "    X_test.drop(columns=categorical_cols, inplace=True)\n",
    "\n",
    "    # Concatenate encoded data with remaining data\n",
    "    X_train_ohe = pd.concat([X_train.reset_index(drop=True), X_train_ohe_df.reset_index(drop=True)], axis=1)\n",
    "    X_test_ohe = pd.concat([X_test.reset_index(drop=True), X_test_ohe_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return X_train_ohe, X_test_ohe\n",
    "\n",
    "# Apply one-hot encoding to training and test data\n",
    "X_train_ohe, X_test_ohe = one_hot_encode(X_train.copy(), X_test.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in the input DataFrames using OneHotEncoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Training DataFrame containing categorical columns to be encoded.\n",
    "    X_test : pandas DataFrame\n",
    "        Testing DataFrame containing categorical columns to be encoded.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame, pandas DataFrame\n",
    "        DataFrames with categorical columns encoded using one-hot encoding for both training and testing datasets.\n",
    "    \"\"\"\n",
    "    # Select the columns with categorical values in training set\n",
    "    categorical_cols_train = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Select the columns with categorical values in testing set\n",
    "    categorical_cols_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Initialize the encoder\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    # Fit and transform encoder on training data\n",
    "    encoded_array_train = encoder.fit_transform(X_train[categorical_cols_train])\n",
    "    encoded_array_test = encoder.transform(X_test[categorical_cols_test])\n",
    "\n",
    "    # Convert the encoded arrays into DataFrames\n",
    "    encoded_df_train = pd.DataFrame(encoded_array_train, columns=encoder.get_feature_names_out(categorical_cols_train))\n",
    "    encoded_df_test = pd.DataFrame(encoded_array_test, columns=encoder.get_feature_names_out(categorical_cols_test))\n",
    "\n",
    "    # Concatenate the encoded DataFrames with the original DataFrames for both training and testing sets\n",
    "    result_df_train = pd.concat([X_train, encoded_df_train], axis=1)\n",
    "    result_df_test = pd.concat([X_test, encoded_df_test], axis=1)\n",
    "\n",
    "    # Drop the original categorical columns if needed\n",
    "    result_df_train.drop(columns=categorical_cols_train, axis=1, inplace=True)\n",
    "    result_df_test.drop(columns=categorical_cols_test, axis=1, inplace=True)\n",
    "\n",
    "    return result_df_train, result_df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rescaling numeric features with standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_numeric_features(X_train_ohe, X_test_ohe):\n",
    "    \"\"\"\n",
    "    Scale the numeric features in the training and test datasets using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train_ohe (DataFrame): DataFrame containing the training data with both numeric and non-numeric features.\n",
    "    - X_test_ohe (DataFrame): DataFrame containing the test data with both numeric and non-numeric features.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_stdv (DataFrame): DataFrame containing the scaled numeric features concatenated with the non-numeric features for training data.\n",
    "    - X_test_stdv (DataFrame): DataFrame containing the scaled numeric features concatenated with the non-numeric features for test data.\n",
    "    \"\"\"\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Select numeric features for training data\n",
    "    numeric_features_train = X_train_ohe.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Fit and transform the numeric features in the training set\n",
    "    scaled_features_train = scaler.fit_transform(numeric_features_train)\n",
    "\n",
    "    # Convert the scaled training features array back to a DataFrame\n",
    "    scaled_df_train = pd.DataFrame(scaled_features_train, columns=numeric_features_train.columns)\n",
    "\n",
    "    # Select numeric features for test data\n",
    "    numeric_features_test = X_test_ohe.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Transform the numeric features in the test set\n",
    "    scaled_features_test = scaler.transform(numeric_features_test)\n",
    "\n",
    "    # Convert the scaled test features array back to a DataFrame\n",
    "    scaled_df_test = pd.DataFrame(scaled_features_test, columns=numeric_features_test.columns)\n",
    "\n",
    "    # Concatenate scaled numeric features with other columns in the original DataFrames\n",
    "    X_train_stdv = pd.concat([X_train_ohe.drop(numeric_features_train.columns, axis=1), scaled_df_train], axis=1)\n",
    "    X_test_stdv = pd.concat([X_test_ohe.drop(numeric_features_test.columns, axis=1), scaled_df_test], axis=1)\n",
    "\n",
    "    return X_train_stdv, X_test_stdv\n",
    "\n",
    "# Example usage:\n",
    "X_train_stdv, X_test_stdv = scale_numeric_features(X_train_ohe, X_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline: Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def preprocess_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Preprocesses training and test data including imputation, encoding, and scaling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input training DataFrame.\n",
    "    X_test : pandas DataFrame\n",
    "        Input test DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple of pandas DataFrames\n",
    "        Preprocessed training and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Define preprocessing steps for numerical and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    # Fit and transform the preprocessing steps on training data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Convert the processed data into DataFrames\n",
    "    X_train_processed = pd.DataFrame(X_train_processed, columns=numeric_cols.tolist() +\n",
    "                                     preprocessor.named_transformers_['cat']\n",
    "                                     .named_steps['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "    X_test_processed = pd.DataFrame(X_test_processed, columns=numeric_cols.tolist() +\n",
    "                                    preprocessor.named_transformers_['cat']\n",
    "                                    .named_steps['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "\n",
    "    return X_train_processed, X_test_processed\n",
    "\n",
    "# Preprocess training and test data\n",
    "X_train_processed, X_test_processed = preprocess_data(X_train, X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

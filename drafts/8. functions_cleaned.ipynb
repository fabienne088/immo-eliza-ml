{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>property_type</th>\n",
       "      <th>subproperty_type</th>\n",
       "      <th>region</th>\n",
       "      <th>province</th>\n",
       "      <th>locality</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>total_area_sqm</th>\n",
       "      <th>surface_land_sqm</th>\n",
       "      <th>...</th>\n",
       "      <th>fl_garden</th>\n",
       "      <th>garden_sqm</th>\n",
       "      <th>fl_swimming_pool</th>\n",
       "      <th>fl_floodzone</th>\n",
       "      <th>state_building</th>\n",
       "      <th>primary_energy_consumption_sqm</th>\n",
       "      <th>epc</th>\n",
       "      <th>heating_type</th>\n",
       "      <th>fl_double_glazing</th>\n",
       "      <th>cadastral_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225000.0</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>Flanders</td>\n",
       "      <td>Antwerp</td>\n",
       "      <td>Antwerp</td>\n",
       "      <td>2050</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231.0</td>\n",
       "      <td>C</td>\n",
       "      <td>GAS</td>\n",
       "      <td>1</td>\n",
       "      <td>922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>449000.0</td>\n",
       "      <td>HOUSE</td>\n",
       "      <td>HOUSE</td>\n",
       "      <td>Flanders</td>\n",
       "      <td>East Flanders</td>\n",
       "      <td>Gent</td>\n",
       "      <td>9185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>680.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>221.0</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>335000.0</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>Brussels-Capital</td>\n",
       "      <td>Brussels</td>\n",
       "      <td>Brussels</td>\n",
       "      <td>1070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>AS_NEW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GAS</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>501000.0</td>\n",
       "      <td>HOUSE</td>\n",
       "      <td>HOUSE</td>\n",
       "      <td>Flanders</td>\n",
       "      <td>Antwerp</td>\n",
       "      <td>Turnhout</td>\n",
       "      <td>2275</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>982700.0</td>\n",
       "      <td>APARTMENT</td>\n",
       "      <td>DUPLEX</td>\n",
       "      <td>Wallonia</td>\n",
       "      <td>Walloon Brabant</td>\n",
       "      <td>Nivelles</td>\n",
       "      <td>1410</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AS_NEW</td>\n",
       "      <td>19.0</td>\n",
       "      <td>A+</td>\n",
       "      <td>GAS</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      price property_type subproperty_type            region         province  \\\n",
       "0  225000.0     APARTMENT        APARTMENT          Flanders          Antwerp   \n",
       "1  449000.0         HOUSE            HOUSE          Flanders    East Flanders   \n",
       "2  335000.0     APARTMENT        APARTMENT  Brussels-Capital         Brussels   \n",
       "3  501000.0         HOUSE            HOUSE          Flanders          Antwerp   \n",
       "4  982700.0     APARTMENT           DUPLEX          Wallonia  Walloon Brabant   \n",
       "\n",
       "   locality  zip_code  construction_year  total_area_sqm  surface_land_sqm  \\\n",
       "0   Antwerp      2050             1963.0           100.0               NaN   \n",
       "1      Gent      9185                NaN             NaN             680.0   \n",
       "2  Brussels      1070                NaN           142.0               NaN   \n",
       "3  Turnhout      2275             2024.0           187.0             505.0   \n",
       "4  Nivelles      1410             2022.0           169.0               NaN   \n",
       "\n",
       "   ...  fl_garden  garden_sqm fl_swimming_pool  fl_floodzone  state_building  \\\n",
       "0  ...          0         0.0                0             0             NaN   \n",
       "1  ...          0         0.0                0             0             NaN   \n",
       "2  ...          0         0.0                0             1          AS_NEW   \n",
       "3  ...          0         0.0                0             1             NaN   \n",
       "4  ...          1       142.0                0             0          AS_NEW   \n",
       "\n",
       "   primary_energy_consumption_sqm  epc  heating_type  fl_double_glazing  \\\n",
       "0                           231.0    C           GAS                  1   \n",
       "1                           221.0    C           NaN                  1   \n",
       "2                             NaN  NaN           GAS                  0   \n",
       "3                            99.0    A           NaN                  0   \n",
       "4                            19.0   A+           GAS                  0   \n",
       "\n",
       "   cadastral_income  \n",
       "0             922.0  \n",
       "1             406.0  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 37211 entries, 1 to 75506\n",
      "Data columns (total 27 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   price                           37211 non-null  float64\n",
      " 1   property_type                   37211 non-null  object \n",
      " 2   subproperty_type                37211 non-null  object \n",
      " 3   region                          37211 non-null  object \n",
      " 4   province                        37211 non-null  object \n",
      " 5   locality                        37211 non-null  object \n",
      " 6   zip_code                        37211 non-null  int64  \n",
      " 7   construction_year               19969 non-null  float64\n",
      " 8   total_area_sqm                  32360 non-null  float64\n",
      " 9   surface_land_sqm                37211 non-null  float64\n",
      " 10  nbr_frontages                   29563 non-null  float64\n",
      " 11  nbr_bedrooms                    37211 non-null  float64\n",
      " 12  equipped_kitchen                20753 non-null  object \n",
      " 13  fl_furnished                    37211 non-null  int64  \n",
      " 14  fl_open_fire                    37211 non-null  int64  \n",
      " 15  fl_terrace                      37211 non-null  int64  \n",
      " 16  terrace_sqm                     28393 non-null  float64\n",
      " 17  fl_garden                       37211 non-null  int64  \n",
      " 18  garden_sqm                      34850 non-null  float64\n",
      " 19  fl_swimming_pool                37211 non-null  int64  \n",
      " 20  fl_floodzone                    37211 non-null  int64  \n",
      " 21  state_building                  25682 non-null  object \n",
      " 22  primary_energy_consumption_sqm  27141 non-null  float64\n",
      " 23  epc                             28490 non-null  object \n",
      " 24  heating_type                    23935 non-null  object \n",
      " 25  fl_double_glazing               37211 non-null  int64  \n",
      " 26  cadastral_income                20043 non-null  float64\n",
      "dtypes: float64(10), int64(8), object(9)\n",
      "memory usage: 7.9+ MB\n",
      "None\n",
      "(37211, 27)\n",
      "['Gent' 'Turnhout' 'Halle-Vilvoorde' 'Brugge' 'Sint-Niklaas' 'Charleroi'\n",
      " 'Veurne' 'LiÃ¨ge' 'Brussels' 'Dendermonde' 'Bastogne' 'Mons' 'Tournai'\n",
      " 'Nivelles' 'Aalst' 'Oudenaarde' 'Philippeville' 'Leuven' 'Dinant' 'Ieper'\n",
      " 'Kortrijk' 'Antwerp' 'Huy' 'Marche-en-Famenne' 'Mouscron' 'Verviers'\n",
      " 'Diksmuide' 'Soignies' 'Mechelen' 'Oostend' 'Namur' 'Hasselt' 'Tongeren'\n",
      " 'Arlon' 'NeufchÃ¢teau' 'Thuin' 'Waremme' 'Virton' 'Roeselare' 'Ath'\n",
      " 'Maaseik' 'Tielt' 'Eeklo']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "construction_year                 17242\n",
       "cadastral_income                  17168\n",
       "equipped_kitchen                  16458\n",
       "heating_type                      13276\n",
       "state_building                    11529\n",
       "primary_energy_consumption_sqm    10070\n",
       "terrace_sqm                        8818\n",
       "epc                                8721\n",
       "nbr_frontages                      7648\n",
       "total_area_sqm                     4851\n",
       "garden_sqm                         2361\n",
       "fl_double_glazing                     0\n",
       "fl_floodzone                          0\n",
       "fl_swimming_pool                      0\n",
       "fl_garden                             0\n",
       "price                                 0\n",
       "fl_terrace                            0\n",
       "fl_open_fire                          0\n",
       "property_type                         0\n",
       "nbr_bedrooms                          0\n",
       "surface_land_sqm                      0\n",
       "zip_code                              0\n",
       "locality                              0\n",
       "province                              0\n",
       "region                                0\n",
       "subproperty_type                      0\n",
       "fl_furnished                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file\n",
    "df = pd.read_csv(\"../data/cleaned_properties.csv\")\n",
    "\n",
    "# Display the head\n",
    "display(df.head())\n",
    "df.shape\n",
    "df.columns\n",
    "\n",
    "# Filter the DataFrame for values APARTMENT and APARTMENT_BLOCK\n",
    "df_house1 = df[df[\"property_type\"] == \"HOUSE\"]\n",
    "df_house2 = df_house1[df_house1['subproperty_type'] != 'APARTMENT_BLOCK']\n",
    "\n",
    "df_house = df[(df[\"property_type\"] == \"HOUSE\") & (df['subproperty_type'] != 'APARTMENT_BLOCK')]\n",
    "\n",
    "df_house.head()\n",
    "print(df_house.info())\n",
    "print(df_house.shape)\n",
    "\n",
    "df_house[\"subproperty_type\"].unique()\n",
    "print(df_house[\"locality\"].unique())\n",
    "df_house.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating variables X and y: define the target and the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (37211, 20)\n",
      "y-shape:  (37211,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29768 entries, 8081 to 31954\n",
      "Data columns (total 20 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   region                          29768 non-null  object \n",
      " 1   province                        29768 non-null  object \n",
      " 2   total_area_sqm                  25900 non-null  float64\n",
      " 3   surface_land_sqm                29768 non-null  float64\n",
      " 4   nbr_frontages                   23670 non-null  float64\n",
      " 5   nbr_bedrooms                    29768 non-null  float64\n",
      " 6   equipped_kitchen                16580 non-null  object \n",
      " 7   fl_furnished                    29768 non-null  int64  \n",
      " 8   fl_open_fire                    29768 non-null  int64  \n",
      " 9   fl_terrace                      29768 non-null  int64  \n",
      " 10  terrace_sqm                     22712 non-null  float64\n",
      " 11  fl_garden                       29768 non-null  int64  \n",
      " 12  garden_sqm                      27915 non-null  float64\n",
      " 13  fl_swimming_pool                29768 non-null  int64  \n",
      " 14  fl_floodzone                    29768 non-null  int64  \n",
      " 15  state_building                  20592 non-null  object \n",
      " 16  primary_energy_consumption_sqm  21674 non-null  float64\n",
      " 17  epc                             22766 non-null  object \n",
      " 18  heating_type                    19124 non-null  object \n",
      " 19  fl_double_glazing               29768 non-null  int64  \n",
      "dtypes: float64(7), int64(7), object(6)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Name X and y (specific columns=subset(houses))\n",
    "X = df_house.drop(columns=['price', 'subproperty_type', 'property_type', 'zip_code', 'locality', 'construction_year', 'cadastral_income'])\n",
    "y = df_house['price']\n",
    "\n",
    "# Print shape\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y-shape: \", y.shape)\n",
    "\n",
    "# Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imputing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values:\n",
    "- numerical: mean\n",
    "- categorical: most frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: Imputing missing values\n",
    "\n",
    "Impute missing values in both X_train and x_test\n",
    "\n",
    "So, the correct order of preprocessing steps is:\n",
    "- Impute missing values\n",
    "- Encode categorical columns\n",
    "- Rescale numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equipped_kitchen                  13188\n",
      "heating_type                      10644\n",
      "state_building                     9176\n",
      "primary_energy_consumption_sqm     8094\n",
      "terrace_sqm                        7056\n",
      "epc                                7002\n",
      "nbr_frontages                      6098\n",
      "total_area_sqm                     3868\n",
      "garden_sqm                         1853\n",
      "fl_floodzone                          0\n",
      "fl_swimming_pool                      0\n",
      "region                                0\n",
      "fl_garden                             0\n",
      "province                              0\n",
      "fl_terrace                            0\n",
      "fl_open_fire                          0\n",
      "fl_furnished                          0\n",
      "nbr_bedrooms                          0\n",
      "surface_land_sqm                      0\n",
      "fl_double_glazing                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def impute_data(X_train):\n",
    "    \"\"\"\n",
    "    Imputes missing values in both numerical and categorical columns of the input DataFrame using SimpleImputer.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input DataFrame containing columns with missing values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Select columns with numerical and categorical data\n",
    "    numeric_cols = X_train.select_dtypes(exclude='object').columns.tolist()\n",
    "    categorical_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Impute missing values for numerical columns\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')  \n",
    "    X_train[numeric_cols] = numeric_imputer.fit_transform(X_train[numeric_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')  \n",
    "    X_train[categorical_cols] = categorical_imputer.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "    return X_train\n",
    "\n",
    "# Example usage:\n",
    "#X_train_imputed = impute_data(X_train)\n",
    "print(X_train.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def impute_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Imputes missing values in both numerical and categorical columns of the input DataFrames using SimpleImputer.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Training DataFrame containing columns with missing values.\n",
    "    X_test : pandas DataFrame\n",
    "        Testing DataFrame containing columns with missing values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Training and testing DataFrames with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Select columns with numerical and categorical data in training set\n",
    "    numeric_cols_train = X_train.select_dtypes(exclude='object').columns.tolist()\n",
    "    categorical_cols_train = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Select columns with numerical and categorical data in testing set\n",
    "    numeric_cols_test = X_test.select_dtypes(exclude='object').columns.tolist()\n",
    "    categorical_cols_test = X_test.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Impute missing values for numerical columns in training set\n",
    "    numeric_imputer_train = SimpleImputer(strategy='mean')  \n",
    "    X_train[numeric_cols_train] = numeric_imputer_train.fit_transform(X_train[numeric_cols_train])\n",
    "\n",
    "    # Impute missing values for categorical columns in training set\n",
    "    categorical_imputer_train = SimpleImputer(strategy='most_frequent')  \n",
    "    X_train[categorical_cols_train] = categorical_imputer_train.fit_transform(X_train[categorical_cols_train])\n",
    "\n",
    "    # Impute missing values for numerical columns in testing set\n",
    "    numeric_imputer_test = SimpleImputer(strategy='mean')  \n",
    "    X_test[numeric_cols_test] = numeric_imputer_test.fit_transform(X_test[numeric_cols_test])\n",
    "\n",
    "    # Impute missing values for categorical columns in testing set\n",
    "    categorical_imputer_test = SimpleImputer(strategy='most_frequent')  \n",
    "    X_test[categorical_cols_test] = categorical_imputer_test.fit_transform(X_test[categorical_cols_test])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "# Apply imputing to training and test data\n",
    "X_train_encoded, X_test_encoded = impute_data(X_train.copy(), X_test.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: Encode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_data(X_train):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in the input DataFrame using OneHotEncoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input DataFrame containing categorical columns to be encoded.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        DataFrame with categorical columns encoded using one-hot encoding.\n",
    "    \"\"\"\n",
    "    # Select the columns with categorical values\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Initialize the encoder\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_array = encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "    # Convert the encoded array into a DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # Concatenate the encoded DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([X_train, encoded_df], axis=1)\n",
    "\n",
    "    # Drop the original categorical columns if needed\n",
    "    result_df.drop(columns=categorical_cols, axis=1, inplace=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "    print(result_df.isna().sum().sort_values(ascending=False)) \n",
    "\n",
    "# Example usage:\n",
    "# X_train_encoded = encode_data(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot_encode(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in the input DataFrames using OneHotEncoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input training DataFrame.\n",
    "    X_test : pandas DataFrame\n",
    "        Input test DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple of pandas DataFrames\n",
    "        Encoded training and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Select the columns with categorical values\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Initialize the encoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Fit and transform on training data\n",
    "    X_train_ohe = encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "    # Transform test data\n",
    "    X_test_ohe = encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "    # Convert the encoded data into DataFrames\n",
    "    X_train_ohe_df = pd.DataFrame(X_train_ohe.toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "    X_test_ohe_df = pd.DataFrame(X_test_ohe.toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # Drop original categorical columns from both training and test data\n",
    "    X_train.drop(columns=categorical_cols, inplace=True)\n",
    "    X_test.drop(columns=categorical_cols, inplace=True)\n",
    "\n",
    "    # Concatenate encoded data with remaining data\n",
    "    X_train_ohe = pd.concat([X_train.reset_index(drop=True), X_train_ohe_df.reset_index(drop=True)], axis=1)\n",
    "    X_test_ohe = pd.concat([X_test.reset_index(drop=True), X_test_ohe_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return X_train_ohe, X_test_ohe\n",
    "\n",
    "# Apply one-hot encoding to training and test data\n",
    "X_train_ohe, X_test_ohe = one_hot_encode(X_train.copy(), X_test.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in the input DataFrames using OneHotEncoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Training DataFrame containing categorical columns to be encoded.\n",
    "    X_test : pandas DataFrame\n",
    "        Testing DataFrame containing categorical columns to be encoded.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame, pandas DataFrame\n",
    "        DataFrames with categorical columns encoded using one-hot encoding for both training and testing datasets.\n",
    "    \"\"\"\n",
    "    # Select the columns with categorical values in training set\n",
    "    categorical_cols_train = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Select the columns with categorical values in testing set\n",
    "    categorical_cols_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Initialize the encoder\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    # Fit and transform encoder on training data\n",
    "    encoded_array_train = encoder.fit_transform(X_train[categorical_cols_train])\n",
    "    encoded_array_test = encoder.transform(X_test[categorical_cols_test])\n",
    "\n",
    "    # Convert the encoded arrays into DataFrames\n",
    "    encoded_df_train = pd.DataFrame(encoded_array_train, columns=encoder.get_feature_names_out(categorical_cols_train))\n",
    "    encoded_df_test = pd.DataFrame(encoded_array_test, columns=encoder.get_feature_names_out(categorical_cols_test))\n",
    "\n",
    "    # Concatenate the encoded DataFrames with the original DataFrames for both training and testing sets\n",
    "    result_df_train = pd.concat([X_train, encoded_df_train], axis=1)\n",
    "    result_df_test = pd.concat([X_test, encoded_df_test], axis=1)\n",
    "\n",
    "    # Drop the original categorical columns if needed\n",
    "    result_df_train.drop(columns=categorical_cols_train, axis=1, inplace=True)\n",
    "    result_df_test.drop(columns=categorical_cols_test, axis=1, inplace=True)\n",
    "\n",
    "    return result_df_train, result_df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rescaling numeric features with standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_numeric_features(X_train_ohe, X_test_ohe):\n",
    "    \"\"\"\n",
    "    Scale the numeric features in the training and test datasets using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train_ohe (DataFrame): DataFrame containing the training data with both numeric and non-numeric features.\n",
    "    - X_test_ohe (DataFrame): DataFrame containing the test data with both numeric and non-numeric features.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_stdv (DataFrame): DataFrame containing the scaled numeric features concatenated with the non-numeric features for training data.\n",
    "    - X_test_stdv (DataFrame): DataFrame containing the scaled numeric features concatenated with the non-numeric features for test data.\n",
    "    \"\"\"\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Select numeric features for training data\n",
    "    numeric_features_train = X_train_ohe.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Fit and transform the numeric features in the training set\n",
    "    scaled_features_train = scaler.fit_transform(numeric_features_train)\n",
    "\n",
    "    # Convert the scaled training features array back to a DataFrame\n",
    "    scaled_df_train = pd.DataFrame(scaled_features_train, columns=numeric_features_train.columns)\n",
    "\n",
    "    # Select numeric features for test data\n",
    "    numeric_features_test = X_test_ohe.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Transform the numeric features in the test set\n",
    "    scaled_features_test = scaler.transform(numeric_features_test)\n",
    "\n",
    "    # Convert the scaled test features array back to a DataFrame\n",
    "    scaled_df_test = pd.DataFrame(scaled_features_test, columns=numeric_features_test.columns)\n",
    "\n",
    "    # Concatenate scaled numeric features with other columns in the original DataFrames\n",
    "    X_train_stdv = pd.concat([X_train_ohe.drop(numeric_features_train.columns, axis=1), scaled_df_train], axis=1)\n",
    "    X_test_stdv = pd.concat([X_test_ohe.drop(numeric_features_test.columns, axis=1), scaled_df_test], axis=1)\n",
    "\n",
    "    return X_train_stdv, X_test_stdv\n",
    "\n",
    "# Example usage:\n",
    "X_train_stdv, X_test_stdv = scale_numeric_features(X_train_ohe, X_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline: Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def preprocess_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Preprocesses training and test data including imputation, encoding, and scaling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input training DataFrame.\n",
    "    X_test : pandas DataFrame\n",
    "        Input test DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple of pandas DataFrames\n",
    "        Preprocessed training and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Define preprocessing steps for numerical and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    # Fit and transform the preprocessing steps on training data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Convert the processed data into DataFrames\n",
    "    X_train_processed = pd.DataFrame(X_train_processed, columns=numeric_cols.tolist() +\n",
    "                                     preprocessor.named_transformers_['cat']\n",
    "                                     .named_steps['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "    X_test_processed = pd.DataFrame(X_test_processed, columns=numeric_cols.tolist() +\n",
    "                                    preprocessor.named_transformers_['cat']\n",
    "                                    .named_steps['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "\n",
    "    return X_train_processed, X_test_processed\n",
    "\n",
    "# Preprocess training and test data\n",
    "X_train_processed, X_test_processed = preprocess_data(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train_processed, X_test_processed, preprocessor\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#  Call preprocess_data\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m X_train_processed, X_test_processed \u001b[38;5;241m=\u001b[39m preprocess_data(X_train, X_test)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Save the object to a file\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocess.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "    \"\"\"Preprocesses training and test data including imputation, encoding, and scaling.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (DataFrame): Input training DataFrame.\n",
    "        X_test (DataFrame): Input test DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple of pandas DataFrames: Preprocessed training and test DataFrames.\n",
    "    \"\"\"    \n",
    "    # Separate numerical and categorical columns\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Define preprocessing steps for numerical and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    # Fit and transform the preprocessing steps on training data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Convert the processed data into DataFrames\n",
    "    X_train_processed = pd.DataFrame(X_train_processed, columns=numeric_cols.tolist() +\n",
    "                                     preprocessor.named_transformers_['cat']\n",
    "                                     .named_steps['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "    X_test_processed = pd.DataFrame(X_test_processed, columns=numeric_cols.tolist() +\n",
    "                                    preprocessor.named_transformers_['cat']\n",
    "                                    .named_steps['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "\n",
    "    return X_train_processed, X_test_processed, preprocessor\n",
    "\n",
    "#  Call preprocess_data\n",
    "X_train_processed, X_test_processed = preprocess_data(X_train, X_test)\n",
    "\n",
    "# Save the object to a file\n",
    "with open('preprocess.pkl', 'wb') as file:\n",
    "    pickle.dump(preprocess_data, file)\n",
    "\n",
    "# Save the object to a file\n",
    "with open('preprocessor.pkl', 'wb') as file:\n",
    "    pickle.dump(preprocessor, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(transformers=[('num',\n",
      "                                 Pipeline(steps=[('imputer', SimpleImputer()),\n",
      "                                                 ('scaler', StandardScaler())]),\n",
      "                                 Index(['total_area_sqm', 'surface_land_sqm', 'nbr_frontages', 'nbr_bedrooms',\n",
      "       'fl_furnished', 'fl_open_fire', 'fl_terrace', 'terrace_sqm',\n",
      "       'fl_garden', 'garden_sqm', 'fl_swimming_pool', 'fl_floodzone',\n",
      "       'primary_energy_consumption_sqm', 'fl_double_glazing'],\n",
      "      dtype='object')),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('onehot',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                 Index(['region', 'province', 'equipped_kitchen', 'state_building', 'epc',\n",
      "       'heating_type'],\n",
      "      dtype='object'))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_data(X_train):\n",
    "    \"\"\" Preprocesses training and test data including imputation, encoding, and scaling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input training DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Preprocessed training DataFrame.\n",
    "    \"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Define preprocessing steps for numerical and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    # Fit and transform the preprocessing steps on training data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "    # Convert the processed data into DataFrames\n",
    "    X_train_processed = pd.DataFrame(X_train_processed, columns=numeric_cols.tolist() +\n",
    "                                     preprocessor.named_transformers_['cat']\n",
    "                                     .named_steps['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "    print(preprocessor)\n",
    "    return X_train_processed, preprocessor\n",
    "\n",
    "\n",
    "def preprocess_data_for_test(X_test, preprocessor):\n",
    "    \"\"\" Preprocesses test data including imputation, encoding, and scaling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_test : pandas DataFrame\n",
    "        Input test DataFrame.\n",
    "    preprocessor : sklearn ColumnTransformer\n",
    "        Fitted preprocessor used to transform the test data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Preprocessed test DataFrame.\n",
    "    \"\"\"\n",
    "    # Transform the test data using the fitted preprocessor\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Get the column names for the transformed data\n",
    "    numeric_cols = X_test.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_test.select_dtypes(include=['object']).columns\n",
    "\n",
    "    transformed_columns = numeric_cols.tolist() + \\\n",
    "                          preprocessor.named_transformers_['cat'].named_steps['onehot'] \\\n",
    "                          .get_feature_names_out(categorical_cols).tolist()\n",
    "\n",
    "    # Convert the processed data into a DataFrame\n",
    "    X_test_processed = pd.DataFrame(X_test_processed, columns=transformed_columns)\n",
    "\n",
    "    return X_test_processed\n",
    "\n",
    "# Preprocess training data and obtain the preprocessor object\n",
    "X_train_processed, preprocessor = preprocess_data(X_train)\n",
    "\n",
    "# Preprocess test data using the preprocessor object\n",
    "X_test_processed = preprocess_data_for_test(X_test, preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7443 entries, 25920 to 74318\n",
      "Data columns (total 20 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   region                          7443 non-null   object \n",
      " 1   province                        7443 non-null   object \n",
      " 2   total_area_sqm                  6460 non-null   float64\n",
      " 3   surface_land_sqm                7443 non-null   float64\n",
      " 4   nbr_frontages                   5893 non-null   float64\n",
      " 5   nbr_bedrooms                    7443 non-null   float64\n",
      " 6   equipped_kitchen                4173 non-null   object \n",
      " 7   fl_furnished                    7443 non-null   int64  \n",
      " 8   fl_open_fire                    7443 non-null   int64  \n",
      " 9   fl_terrace                      7443 non-null   int64  \n",
      " 10  terrace_sqm                     5681 non-null   float64\n",
      " 11  fl_garden                       7443 non-null   int64  \n",
      " 12  garden_sqm                      6935 non-null   float64\n",
      " 13  fl_swimming_pool                7443 non-null   int64  \n",
      " 14  fl_floodzone                    7443 non-null   int64  \n",
      " 15  state_building                  5090 non-null   object \n",
      " 16  primary_energy_consumption_sqm  5467 non-null   float64\n",
      " 17  epc                             5724 non-null   object \n",
      " 18  heating_type                    4811 non-null   object \n",
      " 19  fl_double_glazing               7443 non-null   int64  \n",
      "dtypes: float64(7), int64(7), object(6)\n",
      "memory usage: 1.2+ MB\n",
      "None\n",
      "['region', 'province', 'total_area_sqm', 'surface_land_sqm', 'nbr_frontages', 'nbr_bedrooms', 'equipped_kitchen', 'fl_furnished', 'fl_open_fire', 'fl_terrace', 'terrace_sqm', 'fl_garden', 'garden_sqm', 'fl_swimming_pool', 'fl_floodzone', 'state_building', 'primary_energy_consumption_sqm', 'epc', 'heating_type', 'fl_double_glazing']\n"
     ]
    }
   ],
   "source": [
    "print(X_test.info())\n",
    "print(X_test.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the preprocessor to a file\n",
    "with open('preprocessor.pkl', 'wb') as file:\n",
    "    pickle.dump(preprocessor, file)\n",
    "\n",
    "# Save the preprocess_data_for_test to a file\n",
    "with open('preprocessing.pkl', 'wb') as file:\n",
    "    pickle.dump(preprocess_data_for_test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for the new DataFrame\n",
    "data = {\n",
    "    'region': ['Flanders', 'Flanders', 'Brussels-Capital', 'Wallonia'],\n",
    "    'province': ['East Flanders', 'Antwerp', 'Brussels', 'Namur'],\n",
    "    'total_area_sqm': [125.5, 185.3, 110.7, 150.2],\n",
    "    'surface_land_sqm': [680, 180, 505, 710],\n",
    "    'nbr_frontages': [3, 4, 2, 3],\n",
    "    'nbr_bedrooms': [3, 4, 2, 3],\n",
    "    'equipped_kitchen': ['nan', 'HYPER_EQUIPPED', 'INSTALLED', 'USA_UNINSTALLED'],\n",
    "    'fl_furnished': [1, 0, 1, 0],\n",
    "    'fl_open_fire': [1, 0, 1, 0],\n",
    "    'fl_terrace': [1, 1, 0, 1],\n",
    "    'terrace_sqm': [20, 25, None, 30],\n",
    "    'fl_garden': [1, 0, 1, 0],\n",
    "    'garden_sqm': [50, None, 60, None],\n",
    "    'fl_swimming_pool': [1, 0, 1, 0],\n",
    "    'fl_floodzone': [0, 1, 0, 1],\n",
    "    'state_building': ['GOOD', 'AS_NEW', None, 'GOOD'],\n",
    "    'primary_energy_consumption_sqm': [150, None, 280, None],\n",
    "    'epc': ['A', 'B', 'C', 'D'],\n",
    "    'heating_type': ['GAS', 'ELECTRIC', 'FUELOIL', None],\n",
    "    'fl_double_glazing': [1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Create a new DataFrame\n",
    "new_data_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(new_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in column 'region': ['Flanders' 'Wallonia' 'Brussels-Capital']\n",
      "Unique values in column 'province': ['East Flanders' 'Antwerp' 'Flemish Brabant' 'West Flanders' 'Hainaut'\n",
      " 'LiÃ¨ge' 'Brussels' 'Luxembourg' 'Walloon Brabant' 'Namur' 'Limburg']\n",
      "Unique values in column 'total_area_sqm': [       nan 1.8700e+02 1.5500e+02 2.7700e+02 3.0900e+02 1.5000e+02\n",
      " 5.8300e+02 1.8500e+02 1.6000e+02 1.2700e+02 1.0900e+02 1.8000e+02\n",
      " 6.0000e+02 2.2000e+02 7.9000e+01 2.0400e+02 1.2000e+02 4.8000e+02\n",
      " 1.3000e+02 1.0000e+02 2.0000e+02 1.8100e+02 1.6600e+02 1.4600e+02\n",
      " 8.6000e+01 1.4800e+02 2.1300e+02 2.4300e+02 2.3500e+02 2.3100e+02\n",
      " 2.2900e+02 4.3100e+02 1.6200e+02 2.5000e+02 1.5000e+03 1.4500e+02\n",
      " 1.6800e+02 2.5900e+02 2.1800e+02 1.9100e+02 9.0000e+02 1.3300e+02\n",
      " 1.4400e+02 1.6100e+02 1.7500e+02 1.8300e+02 1.8600e+02 2.0600e+02\n",
      " 1.9300e+02 1.7000e+02 1.5600e+02 1.5800e+02 5.3700e+02 2.5500e+02\n",
      " 2.0500e+02 1.8400e+02 9.5000e+01 3.3000e+02 1.4300e+02 2.5100e+02\n",
      " 2.8100e+02 2.9000e+02 1.7900e+02 1.6700e+02 1.0500e+02 1.4100e+02\n",
      " 1.2400e+02 3.8000e+02 2.1820e+03 1.3400e+02 1.3600e+02 2.2600e+02\n",
      " 1.2800e+02 1.9000e+02 2.8600e+02 8.4000e+02 1.3500e+02 1.8800e+02\n",
      " 1.8900e+02 4.0000e+02 2.0800e+02 4.5400e+02 2.4100e+02 5.3200e+02\n",
      " 6.0000e+01 1.1100e+02 1.2900e+02 2.4000e+02 2.1500e+02 2.9100e+02\n",
      " 3.8500e+02 1.1200e+02 1.6400e+02 1.8200e+02 2.8500e+02 2.8800e+02\n",
      " 2.8200e+02 2.3400e+02 7.0000e+02 1.3200e+02 9.2000e+01 3.1600e+02\n",
      " 1.7700e+02 2.9400e+02 3.5000e+02 2.1100e+02 1.2300e+02 1.1000e+02\n",
      " 9.4000e+01 2.3800e+02 1.7800e+02 2.0300e+02 3.2000e+02 9.0000e+01\n",
      " 2.2500e+02 8.9000e+01 2.3000e+02 6.1000e+02 2.4600e+02 7.6000e+01\n",
      " 1.7600e+02 3.2500e+02 1.5400e+02 2.7000e+02 1.9900e+02 1.6300e+02\n",
      " 3.4600e+02 1.7300e+02 2.9500e+02 1.2500e+02 1.6500e+02 2.6500e+02\n",
      " 2.9800e+02 1.0200e+02 3.5500e+02 1.2600e+02 1.1500e+02 2.7800e+02\n",
      " 2.2300e+02 3.1500e+02 3.5400e+02 2.0900e+02 8.5000e+01 1.4000e+02\n",
      " 2.1000e+02 3.2600e+02 1.0700e+02 1.7400e+02 1.0400e+02 1.5300e+02\n",
      " 1.4200e+02 3.1000e+02 1.0800e+02 1.5200e+02 4.3000e+02 7.4000e+01\n",
      " 2.1200e+02 1.9500e+02 1.1900e+02 3.1700e+02 2.4200e+02 6.1800e+02\n",
      " 2.6000e+02 1.2200e+02 2.6400e+02 2.5700e+02 3.0000e+02 1.6900e+02\n",
      " 2.0700e+02 1.2880e+03 2.1600e+02 1.3700e+02 7.5000e+01 3.5300e+02\n",
      " 1.4700e+02 1.3900e+02 1.9600e+02 2.3600e+02 9.4300e+02 4.3800e+02\n",
      " 2.4900e+02 5.6000e+02 1.9400e+02 2.2400e+02 3.9200e+02 1.4900e+02\n",
      " 7.0000e+01 2.6300e+02 5.4300e+02 1.1300e+02 2.4500e+02 1.9800e+02\n",
      " 1.7000e+03 2.6700e+02 2.0200e+02 8.0000e+01 6.0600e+02 1.1700e+02\n",
      " 5.0000e+02 7.1900e+02 4.4500e+02 3.9000e+02 3.8300e+02 1.7100e+02\n",
      " 1.5100e+02 2.5600e+02 2.1700e+02 1.1400e+02 2.2100e+02 4.6500e+02\n",
      " 3.4500e+02 2.3900e+02 2.3300e+02 1.1600e+02 5.7000e+01 3.4700e+02\n",
      " 2.7400e+02 2.5400e+02 2.8000e+02 3.0300e+02 1.0300e+02 2.7200e+02\n",
      " 1.2100e+02 1.5900e+02 6.2000e+01 1.3800e+02 2.5300e+02 2.1400e+02\n",
      " 2.4800e+02 1.1800e+02 2.6600e+02 1.9200e+02 1.9700e+02 8.2800e+02\n",
      " 1.1270e+03 2.8700e+02 3.7500e+02 2.7900e+02 4.1500e+02 3.3200e+02\n",
      " 2.2200e+02 8.7000e+01 2.9200e+02 1.6450e+03 3.7000e+02 3.9500e+02\n",
      " 3.6500e+02 3.0800e+02 3.5600e+02 1.1200e+03 4.2000e+02 9.7000e+01\n",
      " 3.1300e+02 8.3000e+01 5.5000e+02 3.6600e+02 1.3100e+02 2.0100e+02\n",
      " 3.6000e+02 2.9900e+02 7.9900e+02 4.2500e+02 1.7200e+02 2.8300e+02\n",
      " 4.0200e+02 1.0600e+02 9.3000e+01 3.3100e+02 6.3000e+01 5.7500e+02\n",
      " 3.8900e+02 4.6000e+02 3.3500e+02 1.5700e+02 3.3400e+02 6.5000e+01\n",
      " 4.3500e+02 3.9400e+02 4.5000e+02 2.7100e+02 1.0100e+02 2.4700e+02\n",
      " 6.4000e+01 3.1200e+02 4.7500e+02 2.2800e+02 1.4000e+03 4.9200e+02\n",
      " 5.0800e+02 2.6200e+02 3.2800e+02 4.1800e+02 5.4400e+02 3.4400e+02\n",
      " 2.1900e+02 3.7300e+02 3.3600e+02 1.1040e+03 2.2700e+02 3.4000e+02\n",
      " 3.7600e+02 2.7600e+02 9.9000e+01 4.2600e+02 7.9400e+02 4.5300e+02\n",
      " 3.7800e+02 8.1000e+01 4.4700e+02 1.0000e+03 2.4400e+02 9.6000e+01\n",
      " 8.8000e+01 6.6000e+01 4.8500e+02 3.3800e+02 3.4300e+02 4.0100e+02\n",
      " 7.5000e+02 4.0600e+02 3.1400e+02 8.2000e+01 5.9700e+02 3.2200e+02\n",
      " 2.5800e+02 4.9300e+02 2.8400e+02 5.2000e+02 4.7900e+02 4.7000e+01\n",
      " 4.0000e+01 1.3670e+03 8.4000e+01 3.2400e+02 5.4000e+02 2.6100e+02\n",
      " 3.9900e+02 5.9300e+02 2.9700e+02 7.7500e+02 5.9000e+01 3.8600e+02\n",
      " 3.9700e+02 3.2300e+02 5.7800e+02 2.6800e+02 6.2000e+02 5.1500e+02\n",
      " 3.6700e+02 2.3200e+02 6.8000e+02 4.4600e+02 7.7000e+01 1.2000e+03\n",
      " 3.3300e+02 3.0100e+02 5.8000e+01 8.6400e+02 4.0400e+02 7.1600e+02\n",
      " 8.2700e+02 3.9100e+02 5.6500e+02 4.2400e+02 4.5800e+02 3.2000e+03\n",
      " 5.3800e+02 8.8000e+02 2.5200e+02 4.4200e+02 5.0700e+02 5.5900e+02\n",
      " 5.8700e+02 2.9600e+02 2.7300e+02 6.3600e+02 2.7500e+02 6.6000e+02\n",
      " 4.1000e+02 1.1730e+03 3.9300e+02 3.0600e+02 4.0500e+02 4.8400e+02\n",
      " 4.4400e+02 8.4500e+02 7.1000e+01 4.1900e+02 5.1600e+02 5.9100e+02\n",
      " 4.4000e+02 1.0060e+03 3.2100e+02 5.0300e+02 8.5000e+02 3.3900e+02\n",
      " 4.8000e+01 5.6200e+02 3.5700e+02 1.1000e+04 1.4770e+04 4.6200e+02\n",
      " 4.1700e+02 3.1900e+02 3.6900e+02 5.3500e+02 4.5200e+02 9.8000e+01\n",
      " 9.6700e+02 4.2700e+02 1.1550e+03 5.4200e+02 3.0500e+02 6.2700e+02\n",
      " 5.1300e+02 4.2200e+02 3.8200e+02 5.4000e+01 2.3700e+02 6.8000e+01\n",
      " 4.6800e+02 8.4300e+02 1.1500e+03 2.2020e+03 2.6900e+02 4.1600e+02\n",
      " 5.9400e+02 3.6300e+02 4.3300e+02 4.1300e+02 9.1000e+01 3.6100e+02\n",
      " 3.1800e+02 8.6000e+02 3.6400e+02 5.3000e+01 7.8000e+01 7.8000e+02\n",
      " 7.2000e+02 3.7100e+02 3.8700e+02 3.0200e+02 8.0000e+02 4.5000e+01\n",
      " 5.0000e+01 4.7100e+02 6.8900e+02 9.3000e+02 3.8100e+02 2.9300e+02\n",
      " 7.3000e+01 6.7000e+01 3.5900e+02 7.4700e+02 3.8800e+02 3.4200e+02\n",
      " 8.0200e+02 1.5150e+03 3.5200e+02 4.2800e+02 6.5000e+02 5.1000e+01\n",
      " 3.5100e+02 5.0600e+02 7.0900e+02 9.7000e+02 5.1000e+02 5.2000e+01\n",
      " 4.6900e+02 1.0070e+03 4.8600e+02 4.6300e+02 6.9000e+01 3.4900e+02\n",
      " 8.2900e+02 6.1700e+02 4.5600e+02 5.2200e+02 5.3300e+02 1.8750e+03\n",
      " 7.4800e+02 3.4100e+02 4.2900e+02 3.0400e+02 3.6800e+02 3.3700e+02\n",
      " 6.4700e+02 4.1100e+02 8.9000e+02 4.3900e+02 9.1700e+02 4.4800e+02\n",
      " 4.5500e+02 4.7600e+02 7.1800e+02 6.7000e+02 9.8800e+02 5.7200e+02\n",
      " 7.1300e+02 4.9100e+02 6.5500e+02 4.7000e+02 5.4900e+02 5.7300e+02\n",
      " 9.3900e+02 3.6200e+02 4.0900e+02 4.3000e+01 4.9000e+02 3.5000e+01\n",
      " 8.5500e+02 5.6400e+02 5.1900e+02 5.8000e+02 4.2300e+02 6.1900e+02\n",
      " 9.7400e+02 3.7000e+01 5.3600e+02 5.5000e+01 7.4000e+02 5.0500e+02\n",
      " 7.6700e+02 8.5400e+02 1.0110e+03 1.7500e+03 8.4100e+02 7.0600e+02\n",
      " 7.7360e+03 8.3000e+02 1.0400e+03 7.7000e+02 5.7400e+02 6.1000e+01\n",
      " 5.6800e+02 7.0200e+02 6.6300e+02 4.8300e+02 6.6700e+02 8.9500e+02\n",
      " 1.9000e+03 5.4600e+02 3.9000e+01 3.7700e+02 4.9000e+01 5.2500e+02\n",
      " 2.8900e+02 1.1150e+03 7.8800e+02 4.0300e+02 3.8400e+02 8.5700e+02\n",
      " 3.4800e+02 4.9400e+02 3.1100e+02 9.0800e+02 4.9600e+02 4.9900e+02\n",
      " 1.0560e+03 4.5900e+02 3.9800e+02 9.6000e+02 1.3600e+03 2.7730e+03\n",
      " 1.5500e+03 1.0150e+03 4.5700e+02 3.7900e+02 1.1000e+03 4.1200e+02\n",
      " 9.6100e+02 8.7500e+02 7.1000e+02 6.8600e+02 5.5100e+02 3.0700e+02\n",
      " 9.2500e+02 4.2000e+01 6.1400e+02 7.9000e+02 4.3700e+02 5.6000e+01\n",
      " 5.4500e+02 7.6100e+02 5.5500e+02 3.2900e+02 8.7000e+02 4.7800e+02\n",
      " 5.3000e+02 1.0390e+03 6.5200e+02 4.8900e+02 9.5900e+02 5.7700e+02\n",
      " 4.0800e+02 4.6700e+02 6.3200e+02 3.9600e+02 2.3000e+03 9.9800e+02\n",
      " 4.3200e+02 1.8890e+03 1.0250e+03 5.6300e+02 9.2600e+03 6.7100e+02\n",
      " 7.2900e+02 6.4500e+02 8.7800e+02 4.8800e+02 6.0700e+02 7.4400e+02\n",
      " 6.9000e+02 2.1590e+03 4.4300e+02 1.3210e+03 5.1400e+02 3.7400e+02\n",
      " 4.8100e+02 3.5800e+02 5.5300e+02 7.0500e+02 7.3200e+02 6.1500e+02\n",
      " 8.6500e+02 7.3000e+02 6.0500e+02 6.6100e+02 8.8500e+02 4.6100e+02\n",
      " 1.0920e+03 5.0900e+02 3.2700e+02 6.4600e+02 1.4900e+03 5.2100e+02\n",
      " 5.4800e+02 5.2900e+02 4.2100e+02 5.0400e+02 4.8700e+02 6.0200e+02\n",
      " 9.0700e+02 1.1900e+03 5.0100e+02 5.9500e+02 7.2500e+02 4.3400e+02\n",
      " 4.7200e+02 9.5000e+02 7.2700e+02 1.3000e+03 1.2650e+03 1.2400e+03\n",
      " 6.6200e+02 6.7500e+02 1.0890e+03 9.0200e+02 7.4200e+03 4.7300e+02\n",
      " 1.3500e+03 6.7300e+02 8.3200e+02 6.4000e+02 8.3500e+02 9.9400e+02\n",
      " 9.6900e+02 6.4800e+02 8.0700e+02 5.2800e+02 1.0100e+03 1.0900e+03\n",
      " 7.2000e+01 5.6700e+02 7.1700e+02 1.4610e+03 7.4200e+02 5.3100e+02\n",
      " 4.7700e+02 7.9100e+02 8.2200e+02 8.8700e+02 2.8000e+01 1.6520e+03\n",
      " 4.1000e+01 7.8500e+02 4.0700e+02 4.3600e+02 7.6400e+02 7.1500e+02\n",
      " 7.5700e+02 1.1390e+03 1.1250e+04 5.6600e+02 7.8300e+02 5.1100e+02\n",
      " 1.5348e+04 6.3000e+02 8.6700e+02 8.6800e+02 1.2800e+03 6.0900e+02\n",
      " 5.5600e+02 2.4380e+03 5.8500e+02 1.1600e+03 5.5200e+02 1.6800e+03\n",
      " 7.6000e+02 5.2700e+02 7.5500e+02 8.1000e+02 6.4200e+02 2.0410e+03\n",
      " 3.4000e+01 6.6900e+02 7.2100e+02 6.3900e+02 5.9000e+02 3.0000e+01\n",
      " 5.7600e+02 4.9800e+02 6.9900e+02 5.2300e+02 5.5800e+02 7.3500e+02\n",
      " 7.0000e+00 1.0260e+03 5.8900e+02 1.1570e+03 1.2820e+03 1.4820e+03\n",
      " 6.5700e+02 7.5300e+02 1.7770e+03 5.8800e+02 5.1800e+02 6.9400e+02\n",
      " 7.2800e+02 5.7000e+02 8.5600e+02 1.4500e+03 4.1400e+02 9.7600e+02\n",
      " 1.2040e+03 9.8500e+02 6.3400e+02 5.2600e+02 6.7600e+02 9.8000e+02\n",
      " 1.1890e+03 8.2500e+02 4.6600e+02 7.1200e+02 8.8900e+02 6.3100e+02\n",
      " 7.3900e+02 9.2000e+02 1.0360e+03 7.5800e+02 1.3940e+03 1.1910e+03\n",
      " 1.3800e+03 5.1200e+02 9.8600e+02 1.1250e+03 5.4700e+02 1.0340e+03\n",
      " 1.1650e+03 5.5960e+03 9.5700e+02 1.9940e+03 7.1400e+02 4.9500e+02\n",
      " 9.4400e+02 6.5600e+02 3.8000e+01 3.0000e+03 2.2000e+03 4.4900e+02\n",
      " 9.5400e+02 2.3070e+03 1.1410e+03 7.0300e+02 8.0800e+02 7.9300e+02\n",
      " 1.2380e+03 5.9200e+02 2.9000e+01 2.0000e+03 1.5710e+03 7.4100e+02\n",
      " 5.2400e+02 9.4500e+02 6.2200e+02 6.3300e+02 1.3370e+03 5.6900e+02\n",
      " 2.8180e+03 1.6500e+03 1.3050e+03 1.1050e+03 1.1460e+03 6.2300e+02\n",
      " 1.6000e+03 6.5100e+02 6.2500e+02 9.6800e+02 7.5100e+02 3.7200e+02\n",
      " 6.0800e+02 1.4050e+03 1.0540e+03 4.6400e+02 5.0200e+02 2.4000e+01\n",
      " 9.4000e+02 5.8600e+02 7.2200e+02 7.1100e+02 7.0700e+02 1.3900e+03\n",
      " 1.6870e+03 5.4100e+02 5.6100e+02 6.3700e+02 1.0000e+04 1.8010e+03\n",
      " 4.4100e+02 2.3430e+03 6.4400e+02 2.2950e+03 1.8000e+03 4.9700e+02\n",
      " 8.2000e+02 6.2800e+02 8.4800e+02 7.7100e+02 1.9310e+03 1.0300e+03\n",
      " 1.0160e+03 2.4000e+03 7.4300e+02 6.7200e+02 5.9900e+02 8.8200e+02\n",
      " 6.8200e+02 1.4650e+03 5.1700e+02 8.0300e+02 5.3900e+02 8.2100e+02\n",
      " 3.6000e+01 6.6500e+02 2.7110e+03 6.8300e+02 6.8500e+02 2.8000e+03\n",
      " 2.5000e+01 4.8200e+02 8.7300e+02 4.4000e+01 1.1660e+03 5.3400e+02\n",
      " 3.3900e+03 4.5100e+02 8.1900e+02 3.2000e+01 5.7900e+02 8.8400e+02\n",
      " 1.8160e+03 1.5600e+03 1.3000e+01 1.3850e+03]\n",
      "Unique values in column 'surface_land_sqm': [ 680.  505.  710. ... 2964. 2332. 4660.]\n",
      "Unique values in column 'nbr_frontages': [nan  4.  3.  2.  1.  9.  6. 15.  8.  7. 13. 47.  5. 18.]\n",
      "Unique values in column 'nbr_bedrooms': [  2.   3.   4.   6.   7.   5.  20.   1.  10.   8.   0.  19.   9.  15.\n",
      "  25.  30.  13.  11.  12.  18.  16.  22.  14.  24.  17.  47.  48. 200.\n",
      "  34.  26.  35.  28.  21.  23.  41.  70.  45.  60.]\n",
      "Unique values in column 'equipped_kitchen': [nan 'HYPER_EQUIPPED' 'INSTALLED' 'USA_UNINSTALLED' 'SEMI_EQUIPPED'\n",
      " 'USA_HYPER_EQUIPPED' 'NOT_INSTALLED' 'USA_INSTALLED' 'USA_SEMI_EQUIPPED']\n",
      "Unique values in column 'fl_furnished': [0 1]\n",
      "Unique values in column 'fl_open_fire': [0 1]\n",
      "Unique values in column 'fl_terrace': [0 1]\n",
      "Unique values in column 'terrace_sqm': [0.000e+00       nan 4.000e+01 1.200e+01 2.500e+01 3.800e+01 1.800e+01\n",
      " 3.500e+01 2.000e+01 1.500e+01 6.000e+00 1.000e+00 3.600e+01 3.000e+01\n",
      " 7.000e+01 8.800e+01 2.100e+01 2.800e+01 1.000e+01 5.000e+00 2.700e+01\n",
      " 2.300e+02 9.000e+00 1.700e+01 7.600e+01 1.120e+02 3.700e+01 5.000e+01\n",
      " 1.600e+01 2.200e+01 1.140e+02 3.000e+00 3.100e+01 1.100e+01 1.000e+02\n",
      " 6.000e+01 5.100e+01 5.600e+01 2.400e+01 8.000e+00 2.600e+02 7.000e+00\n",
      " 1.050e+02 1.400e+01 4.400e+01 5.800e+01 2.000e+02 3.200e+01 2.900e+01\n",
      " 6.400e+01 1.100e+02 1.900e+01 8.400e+01 3.300e+01 2.300e+01 1.300e+01\n",
      " 1.600e+02 4.500e+01 4.000e+00 1.020e+02 7.500e+01 3.400e+01 4.200e+01\n",
      " 5.300e+01 1.260e+02 9.200e+01 9.000e+01 1.130e+02 2.600e+01 1.350e+02\n",
      " 5.200e+01 1.200e+02 4.100e+01 8.000e+01 6.800e+01 8.500e+01 3.466e+03\n",
      " 5.500e+01 7.900e+01 9.100e+01 2.100e+02 4.600e+01 4.800e+01 8.200e+01\n",
      " 6.200e+01 4.700e+01 1.320e+02 4.900e+01 1.500e+02 2.020e+02 6.500e+01\n",
      " 1.800e+02 8.300e+01 5.400e+01 7.700e+01 1.420e+02 3.110e+02 5.000e+02\n",
      " 1.750e+02 3.900e+01 9.700e+01 6.700e+01 8.900e+01 4.500e+02 2.320e+02\n",
      " 5.900e+01 9.300e+01 1.700e+02 9.500e+01 1.210e+02 6.300e+01 6.600e+01\n",
      " 4.300e+01 2.200e+02 7.100e+01 1.300e+02 1.610e+02 7.200e+01 2.000e+00\n",
      " 8.700e+01 7.800e+01 1.400e+02 1.000e+03 3.750e+02 1.250e+02 2.800e+02\n",
      " 3.286e+03 5.700e+01 1.160e+02 7.300e+01 1.070e+02 5.500e+02 8.600e+01\n",
      " 1.280e+02 9.800e+01 1.030e+02 6.900e+01 1.090e+02 1.900e+02 3.820e+02\n",
      " 7.400e+01 1.270e+02 1.440e+02 6.100e+01 1.150e+02 8.100e+01 1.080e+02\n",
      " 2.500e+02 3.000e+02 1.390e+02 1.060e+02 5.430e+02 9.900e+01 2.700e+02\n",
      " 2.360e+02 1.710e+02 2.520e+02 2.250e+02 1.450e+02 1.240e+02 3.100e+02\n",
      " 4.550e+02 9.480e+02 1.790e+02 1.110e+02 1.950e+02 1.230e+02 6.000e+02\n",
      " 1.010e+02 9.400e+01 3.120e+02 1.770e+02 1.820e+02 2.771e+03 1.760e+02\n",
      " 1.650e+02 1.180e+02 1.220e+02 2.070e+02 7.000e+02 2.010e+02 4.000e+02\n",
      " 1.840e+02 3.600e+02 4.800e+02]\n",
      "Unique values in column 'fl_garden': [0 1]\n",
      "Unique values in column 'garden_sqm': [   0.   35.   nan ... 1545. 2165.  631.]\n",
      "Unique values in column 'fl_swimming_pool': [0 1]\n",
      "Unique values in column 'fl_floodzone': [0 1]\n",
      "Unique values in column 'state_building': [nan 'AS_NEW' 'GOOD' 'TO_RENOVATE' 'JUST_RENOVATED' 'TO_BE_DONE_UP'\n",
      " 'TO_RESTORE']\n",
      "Unique values in column 'primary_energy_consumption_sqm': [  221.    99.    nan ...   956. 32991.   962.]\n",
      "Unique values in column 'epc': ['C' 'A' nan 'D' 'E' 'B' 'G' 'F' 'A++' 'A+']\n",
      "Unique values in column 'heating_type': [nan 'FUELOIL' 'GAS' 'PELLET' 'ELECTRIC' 'CARBON' 'WOOD' 'SOLAR']\n",
      "Unique values in column 'fl_double_glazing': [1 0]\n"
     ]
    }
   ],
   "source": [
    "columns = df_house[['region', 'province', 'total_area_sqm', 'surface_land_sqm', 'nbr_frontages', 'nbr_bedrooms', 'equipped_kitchen', 'fl_furnished', 'fl_open_fire', 'fl_terrace', 'terrace_sqm', 'fl_garden', 'garden_sqm', 'fl_swimming_pool', 'fl_floodzone', 'state_building', 'primary_energy_consumption_sqm', 'epc', 'heating_type', 'fl_double_glazing']]\n",
    "\n",
    "# See unique values of multiple columns\n",
    "for column in columns:\n",
    "    multi_columns = df_house[column].unique().T\n",
    "    print(f\"Unique values in column '{column}': {multi_columns}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

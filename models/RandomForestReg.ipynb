{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "df = pd.read_csv(\"../data/cleaned_properties.csv\")\n",
    "\n",
    "# Display the head\n",
    "display(df.head())\n",
    "df.shape\n",
    "df.columns\n",
    "\n",
    "# Filter the DataFrame for values APARTMENT and APARTMENT_BLOCK\n",
    "df_house1 = df[df[\"property_type\"] == \"HOUSE\"]\n",
    "df_house2 = df_house1[df_house1['subproperty_type'] != 'APARTMENT_BLOCK']\n",
    "\n",
    "df_house = df[(df[\"property_type\"] == \"HOUSE\") & (df['subproperty_type'] != 'APARTMENT_BLOCK')]\n",
    "\n",
    "df_house.head()\n",
    "print(df_house.info())\n",
    "print(df_house.shape)\n",
    "\n",
    "df_house[\"subproperty_type\"].unique()\n",
    "print(df_house[\"locality\"].unique())\n",
    "df_house.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating variables X and y: define the target and the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name X and y (specific columns=subset(houses))\n",
    "X = df_house.drop(columns=['price', 'subproperty_type', 'property_type', 'zip_code', 'locality', 'construction_year', 'cadastral_income'])\n",
    "y = df_house['price']\n",
    "\n",
    "# Print shape\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y-shape: \", y.shape)\n",
    "\n",
    "# Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "X_train.info()\n",
    "print(X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imputing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values:\n",
    "- numerical: mean\n",
    "- categorical: most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Define DataFrame with missing values\n",
    "df = X_train\n",
    "df2 = X_test\n",
    "\n",
    "# Select columns with numerical and categorical data\n",
    "numeric_cols = df.select_dtypes(exclude='object').columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Impute missing values for numerical columns\n",
    "numeric_imputer = SimpleImputer(strategy='mean')  # You can choose 'mean', 'median', 'most_frequent', or a constant value\n",
    "df[numeric_cols] = numeric_imputer.fit_transform(df[numeric_cols])\n",
    "df2[numeric_cols] = numeric_imputer.transform(df2[numeric_cols])\n",
    "\n",
    "# Impute missing values for categorical columns\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')  # You can choose 'most_frequent', 'constant', or a custom value\n",
    "df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "df2[categorical_cols] = categorical_imputer.transform(df2[categorical_cols])\n",
    "\n",
    "# Display the first few rows of the training and testing sets after imputation\n",
    "print(\"Training set after imputation:\")\n",
    "display(df.head())\n",
    "print(\"\\nTesting set after imputation:\")\n",
    "display(df2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: Imputing missing values for X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def impute_data(X_train):\n",
    "    \"\"\"\n",
    "    Imputes missing values in both numerical and categorical columns of the input DataFrame using SimpleImputer.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input DataFrame containing columns with missing values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Select columns with numerical and categorical data\n",
    "    numeric_cols = X_train.select_dtypes(exclude='object').columns.tolist()\n",
    "    categorical_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Impute missing values for numerical columns\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')  \n",
    "    X_train[numeric_cols] = numeric_imputer.fit_transform(X_train[numeric_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')  \n",
    "    X_train[categorical_cols] = categorical_imputer.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "    #return X_train\n",
    "\n",
    "# Example usage:\n",
    "#X_train_imputed = impute_data(X_train)\n",
    "print(X_train.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: Imputing missing values for X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def impute_missing_values(X_train_imputed, X_test_imputed):\n",
    "    \"\"\"\n",
    "    Imputes missing values in both numerical and categorical columns of training and testing DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    X_train_imputed (DataFrame): The training DataFrame containing missing values.\n",
    "    X_test_imputed (DataFrame): The testing DataFrame containing missing values.\n",
    "\n",
    "    Returns:\n",
    "    X_train_imputed (DataFrame): The training DataFrame with missing values imputed.\n",
    "    X_test_imputed (DataFrame): The testing DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Select columns with numerical and categorical data\n",
    "    numeric_cols = X_train_imputed.select_dtypes(exclude='object').columns.tolist()\n",
    "    categorical_cols = X_train_imputed.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Impute missing values for numerical columns\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')  \n",
    "    X_train_imputed[numeric_cols] = numeric_imputer.fit_transform(X_train_imputed[numeric_cols])\n",
    "    X_test_imputed[numeric_cols] = numeric_imputer.transform(X_test_imputed[numeric_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')  \n",
    "    X_train_imputed[categorical_cols] = categorical_imputer.fit_transform(X_train_imputed[categorical_cols])\n",
    "    X_test_imputed[categorical_cols] = categorical_imputer.transform(X_test_imputed[categorical_cols])\n",
    "\n",
    "    return X_train_imputed, X_test_imputed\n",
    "\n",
    "# Call the function with the train and test DF as input and store the returned imputed DF.\n",
    "X_train_imputed, X_test_imputed = impute_missing_values(X_train.copy(), X_test.copy())\n",
    "\n",
    "# Display the first few rows of the training and testing sets after imputation\n",
    "print(\"Training set after imputation:\")\n",
    "display(X_train_imputed.head())\n",
    "print(\"\\nTesting set after imputation:\")\n",
    "display(X_test_imputed.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical data to a numerical form.\n",
    "\n",
    "Data to convert:  'region', 'province', 'equipped_kitchen', 'state_building', 'epc', 'heating_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns with categorical values\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"columns_to_encode =\", categorical_cols)\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_array = encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "# Convert the encoded array into a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Concatenate the encoded DataFrame with the original DataFrame\n",
    "result_df = pd.concat([X_train, encoded_df], axis=1)\n",
    "\n",
    "# Drop the original categorical columns if needed\n",
    "result_df.drop(columns = categorical_cols, axis=1, inplace=True) \n",
    "\n",
    "print(result_df.info())\n",
    "df.isna().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: Encode_data for X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_data(X_train):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in the input DataFrame using OneHotEncoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input DataFrame containing categorical columns to be encoded.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        DataFrame with categorical columns encoded using one-hot encoding.\n",
    "    \"\"\"\n",
    "    # Select the columns with categorical values\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Initialize the encoder\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_array = encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "    # Convert the encoded array into a DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # Concatenate the encoded DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([X_train, encoded_df], axis=1)\n",
    "\n",
    "    # Drop the original categorical columns if needed\n",
    "    result_df.drop(columns=categorical_cols, axis=1, inplace=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# X_train_encoded = encode_data(X_train_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: one_hot_encode for X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot_encode(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns in the input DataFrames using OneHotEncoder.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input training DataFrame.\n",
    "    X_test : pandas DataFrame\n",
    "        Input test DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple of pandas DataFrames\n",
    "        Encoded training and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Select the columns with categorical values\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Initialize the encoder\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Fit and transform on training data\n",
    "    X_train_ohe = encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "    # Transform test data\n",
    "    X_test_ohe = encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "    # Convert the encoded data into DataFrames\n",
    "    X_train_ohe_df = pd.DataFrame(X_train_ohe.toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "    X_test_ohe_df = pd.DataFrame(X_test_ohe.toarray(), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # Drop original categorical columns from both training and test data\n",
    "    X_train.drop(columns=categorical_cols, inplace=True)\n",
    "    X_test.drop(columns=categorical_cols, inplace=True)\n",
    "\n",
    "    # Concatenate encoded data with remaining data\n",
    "    X_train_ohe = pd.concat([X_train.reset_index(drop=True), X_train_ohe_df.reset_index(drop=True)], axis=1)\n",
    "    X_test_ohe = pd.concat([X_test.reset_index(drop=True), X_test_ohe_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return X_train_ohe, X_test_ohe\n",
    "\n",
    "# Apply one-hot encoding to training and test data\n",
    "X_train_ohe, X_test_ohe = one_hot_encode(X_train_imputed.copy(), X_test_imputed.copy())\n",
    "\n",
    "# Display the first few rows of the training and testing sets after encoding\n",
    "print(\"Training set after encoding:\")\n",
    "display(X_train_ohe.head())\n",
    "print(X_train_ohe.columns.tolist())\n",
    "print(\"\\nTesting set after encoding:\")\n",
    "display(X_test_ohe.head())\n",
    "print(X_test_ohe.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rescaling numeric features with standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# X_train_ohe contains numeric features\n",
    "numeric_features = X_train_ohe.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Fit and transform the numeric features in the training set\n",
    "scaled_features = scaler.fit_transform(numeric_features)\n",
    "\n",
    "# Convert the scaled features array back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=numeric_features.columns)\n",
    "\n",
    "# Concatenate scaled numeric features with other columns in the original DataFrame\n",
    "# Assuming X_train_ohe contains both categorical and numeric features\n",
    "final_df = pd.concat([X_train_ohe.drop(numeric_features.columns, axis=1), scaled_df], axis=1)\n",
    "\n",
    "# Use final_df for your linear regression model\n",
    "\n",
    "# Transform the numeric features in the test set using the parameters learned from the training set\n",
    "X_test_scaled = scaler.transform(X_test_ohe.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# Combine the scaled numeric features with the encoded categorical features\n",
    "# You may need to concatenate these with the encoded categorical features from step 2\n",
    "# Depending on how you've encoded the categorical features\n",
    "\n",
    "# Now, X_train_scaled and X_test_scaled contain the rescaled numeric features\n",
    "# You can use these in your linear regression model\n",
    "\n",
    "# Transform the numeric features in the test set using the parameters learned from the training set\n",
    "X_test_scaled = scaler.transform(X_test_ohe.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "\n",
    "# Combine the scaled numeric features with the encoded categorical features\n",
    "# You may need to concatenate these with the encoded categorical features from step 2\n",
    "# Depending on how you've encoded the categorical features\n",
    "\n",
    "# Now, X_train_scaled and X_test_scaled contain the rescaled numeric features\n",
    "# You can use these in your linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_numeric_features(X_train_ohe, X_test_ohe):\n",
    "    \"\"\"\n",
    "    Scale the numeric features in the training and test datasets using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train_ohe (DataFrame): DataFrame containing the training data with both numeric and non-numeric features.\n",
    "    - X_test_ohe (DataFrame): DataFrame containing the test data with both numeric and non-numeric features.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_stdv (DataFrame): DataFrame containing the scaled numeric features concatenated with the non-numeric features for training data.\n",
    "    - X_test_stdv (DataFrame): DataFrame containing the scaled numeric features concatenated with the non-numeric features for test data.\n",
    "    \"\"\"\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Select numeric features for training data\n",
    "    numeric_features_train = X_train_ohe.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Fit and transform the numeric features in the training set\n",
    "    scaled_features_train = scaler.fit_transform(numeric_features_train)\n",
    "\n",
    "    # Convert the scaled training features array back to a DataFrame\n",
    "    scaled_df_train = pd.DataFrame(scaled_features_train, columns=numeric_features_train.columns)\n",
    "\n",
    "    # Select numeric features for test data\n",
    "    numeric_features_test = X_test_ohe.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Transform the numeric features in the test set\n",
    "    scaled_features_test = scaler.transform(numeric_features_test)\n",
    "\n",
    "    # Convert the scaled test features array back to a DataFrame\n",
    "    scaled_df_test = pd.DataFrame(scaled_features_test, columns=numeric_features_test.columns)\n",
    "\n",
    "    # Concatenate scaled numeric features with other columns in the original DataFrames\n",
    "    X_train_stdv = pd.concat([X_train_ohe.drop(numeric_features_train.columns, axis=1), scaled_df_train], axis=1)\n",
    "    X_test_stdv = pd.concat([X_test_ohe.drop(numeric_features_test.columns, axis=1), scaled_df_test], axis=1)\n",
    "\n",
    "    return X_train_stdv, X_test_stdv\n",
    "\n",
    "# Example usage:\n",
    "X_train_stdv, X_test_stdv = scale_numeric_features(X_train_ohe, X_test_ohe)\n",
    "\n",
    "# Display the first few rows of the training and testing sets after rescaling\n",
    "print(\"Training set after rescaling:\")\n",
    "display(X_train_stdv.head())\n",
    "print(X_train_stdv.columns.tolist())\n",
    "print(\"\\nTesting set after rescaling:\")\n",
    "display(X_test_stdv.head())\n",
    "print(X_test_stdv.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pipeline: Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def preprocess_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Preprocesses training and test data including imputation, encoding, and scaling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Input training DataFrame.\n",
    "    X_test : pandas DataFrame\n",
    "        Input test DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple of pandas DataFrames\n",
    "        Preprocessed training and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Define preprocessing steps for numerical and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "\n",
    "    # Fit and transform the preprocessing steps on training data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Convert the processed data into DataFrames\n",
    "    X_train_processed = pd.DataFrame(X_train_processed, columns=numeric_cols.tolist() +\n",
    "                                     preprocessor.named_transformers_['cat']\n",
    "                                     .named_steps['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "    X_test_processed = pd.DataFrame(X_test_processed, columns=numeric_cols.tolist() +\n",
    "                                    preprocessor.named_transformers_['cat']\n",
    "                                    .named_steps['onehot'].get_feature_names_out(categorical_cols).tolist())\n",
    "\n",
    "    return X_train_processed, X_test_processed\n",
    "\n",
    "# Preprocess training and test data\n",
    "X_train_processed, X_test_processed = preprocess_data(X_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the training and testing sets after preprocessing\n",
    "print(\"Training set after processing:\")\n",
    "display(X_train_processed.head())\n",
    "print(\"\\nTesting set after processing:\")\n",
    "display(X_test_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X_train_ohe, X_test_ohe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RFR model\n",
    "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "\n",
    "# Train the model using the processed X_train and y_train\n",
    "regressor.fit(X_train_ohe, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display score of training model\n",
    "training_score = regressor.score(X_train_ohe, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_test_pred = regressor.predict(X_test_ohe)\n",
    "\n",
    "# Display score of test model\n",
    "testing_score = regressor.score(X_test_ohe, y_test)\n",
    "\n",
    "print(\"Training R^2 score:\", training_score*100, \"%\")\n",
    "print(\"Testing R^2 score:\", testing_score*100, \"%\")\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X_train_stdv, X_test_stdv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RFR model\n",
    "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "\n",
    "# Train the model using the processed X_train and y_train\n",
    "regressor.fit(X_train_stdv, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display score of training model\n",
    "training_score = regressor.score(X_train_stdv, y_train)\n",
    "\n",
    "# Prediction\n",
    "yst_test_pred = regressor.predict(X_test_stdv)\n",
    "\n",
    "# Display score of test model\n",
    "testing_score = regressor.score(X_test_stdv, y_test)\n",
    "\n",
    "print(\"Training R^2 score:\", training_score*100, \"%\")\n",
    "print(\"Testing R^2 score:\", testing_score*100, \"%\")\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, yst_test_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X_train_processed, X_test_processed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RFR model\n",
    "regressor = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "\n",
    "# Train the model using the processed X_train and y_train\n",
    "regressor.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display score of training model\n",
    "training_score = regressor.score(X_train_processed, y_train)\n",
    "\n",
    "# Prediction\n",
    "ypr_test_pred = regressor.predict(X_test_processed)\n",
    "\n",
    "# Display score of test model\n",
    "testing_score = regressor.score(X_test_processed, y_test)\n",
    "\n",
    "print(\"Training R^2 score:\", training_score*100, \"%\")\n",
    "print(\"Testing R^2 score:\", testing_score*100, \"%\")\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, ypr_test_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation of RandomForestRegressor: X_train_processed, X_test_processed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RaFo model: n_estimators = 10?\n",
    "rafo = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "\n",
    "# Train the model using the processed X_train and y_train\n",
    "rafo.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "ypi_pred = rafo.predict(X_test_processed)\n",
    "print(\"Prediction for test set: {}\".format(ypi_pred))\n",
    "\n",
    "# Evaluate the model\n",
    "mae = metrics.mean_absolute_error(y_test, ypi_pred)\n",
    "mse = metrics.mean_squared_error(y_test, ypi_pred)\n",
    "r2 = np.sqrt(metrics.mean_squared_error(y_test, ypi_pred))\n",
    "\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('Mean Square Error:', mse)\n",
    "print('Root Mean Square Error:', r2)\n",
    "\n",
    "#Actual value and the predicted value\n",
    "reg_model_diff = pd.DataFrame({'Actual value': y_test, 'Predicted value': ypi_pred})\n",
    "reg_model_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation of RandomForestRegressor: X_train_processed, X_test_processed**\n",
    "\n",
    "Without n-estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# Create and fit a Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train the model using the processed X_train and y_train\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Display score of training and test model\n",
    "training_score = rf_model.score(X_train_processed, y_train)\n",
    "testing_score = rf_model.score(X_test_processed, y_test)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = rf_model.predict(X_train_processed)\n",
    "y_test_pred = rf_model.predict(X_test_processed)\n",
    "\n",
    "# Evaluation\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Mean Absolute Error (MAE): \n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "train_rmse = root_mean_squared_error(y_train, y_train_pred)\n",
    "test_rmse = root_mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"Random Forest Regressor Model Evaluation:\")\n",
    "print(\"Training score: \", round(training_score*100, 2), \"%\")\n",
    "print(\"Testing score: \", round(testing_score*100, 2), \"%\")\n",
    "print(\"Training R^2 score:\", round(train_r2*100,2),\"%\")\n",
    "print(\"Testing R^2 score:\", round(test_r2*100,2),\"%\")\n",
    "print(\"Training MAE:\", train_mae)\n",
    "print(\"Testing MAE:\", test_mae)\n",
    "print(\"Training MSE:\", train_mse)\n",
    "print(\"Testing MSE:\", test_mse)\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Testing RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mean Absolute Error (MAE): This metric measures the average absolute difference between the predicted values and the actual values. It gives you an idea of how much the predictions deviate from the actual values, on average.\n",
    "\n",
    "2. Mean Squared Error (MSE): MSE measures the average of the squares of the errors, i.e., the average squared difference between the predicted values and the actual values. It penalizes larger errors more heavily than smaller ones.\n",
    "\n",
    "3. Root Mean Squared Error (RMSE): RMSE is simply the square root of MSE. It gives you an idea of the average magnitude of the errors in the predicted values. Since it's in the same units as the target variable, it's often easier to interpret than MSE.\n",
    "\n",
    "4. R-squared (R^2) Score: R^2 score measures the proportion of the variance in the target variable that is predictable from the input features. It ranges from 0 to 1, where 1 indicates a perfect fit and 0 indicates that the model does not explain any of the variance in the target variable better than a horizontal line (the mean of the target variable).\n",
    "\n",
    "So, when you evaluate your Random Forest Regressor using these metrics, you're essentially checking how close its predictions are to the actual values and how well it explains the variance in the target variable. The goal is to have low values for MAE, MSE, and RMSE, and a high value for R^2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
